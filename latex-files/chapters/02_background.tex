Before we dive into related decomposition techniques, it is crucial to have a solid understanding of the research domain. This section introduces important concepts that are used throughout the research domain such as software clustering in general, microservices, the monolith, the kinds of sources of information we can extract from the monolith and clustering algorithms.

\section{Software clustering}\label{s:software_clustering}
Decomposing monolithic software into a suitable set of microservices is based on the idea of software clustering. Software clustering is the general task of grouping software entities based on their interrelationships or similarity. These software entities could represent more detailed functions and their invocations (low-level entities) or more abstract packages, modules and files (high-level entities), depending on the level of detail desired in the clustering \cite{alsarhan2020software}. Software clustering is an old domain of research as one of the first papers published \cite{belady1981system} dates back to 1981. Over the years, software clustering has shown its potential in various application areas, for example, to identify fault proneness or to locate code fragments that implement related functionality \cite{alsarhan2020software}. Another popular software clustering aim is to reduce the system's complexity by splitting up the system into smaller independent chunks of software \cite{wiggerts1997using}. This way, the software becomes less complex, allowing developers to more easily maintain and add new functionality to the software. In this study, we focus on software clustering techniques that aim to identify microservice boundaries in monolithic software.\par
Automatic clustering of software is a challenging task and knows two fundamental problems. The first problem indicates that there are too many unique ways of decomposing software. A paper by \citeauthor{mancoridis1998using} \cite{mancoridis1998using} shows that the number of unique decompositions grows exponential with respect to the number of classes in the source code. This first problem is considered a search problem and is further discussed in Section \ref{s:clustering_algorithms}. The second problem of software clustering is the ambiguity in measuring the quality of a good decomposition. One of the reasons for this is that there is no ideal solution that fits every scenario. Instead, the decomposition quality depends on many factors, such as the organisation structure and the desired granularity. Over the years, many validation metrics have been introduced that aim to quantify either specific microservice decompositions \cite{jin2018functionality} or decompositions in general \cite{mitchell2001craft}. We elaborate on this in Section \ref{s:literature_results}. \par

\section{The monolith}
As mentioned in the introduction, a monolith is a software application in which all the code is combined into a single executable file. \citeauthor{dragoni2017yesterday} \cite{dragoni2017yesterday} defines a monolith as a system \textit{whose modules cannot be executed independently.} The modules are dependent on each other because they rely on shared resources such as databases, files or memory. The monolith suffers from several issues \cite{dragoni2017yesterday}:

\begin{itemize}
    \item hard to maintain when codebase becomes too big
    \item "dependency hell"
    \item a small change requires redeploying the whole system
    \item due to conflicting requirements a 'one-size-fits-all' deployment configuration
    \item limited scalability
    \item technology lock-in for developers
\end{itemize}

To cope with these limitations, the Service-Oriented Architectural (SOA) style has been introduced. In SOAs, software entities interact with other entities via message passing communication \cite{dragoni2017microservices}. Microservices are closely related and have been introduced by the industry due to the fact that the service-oriented paradigm did not give clear requirements on how to design them well \cite{dragoni2017yesterday}. There was a lack of consensus on how to get a well-defined SOA \cite{newman2015building}. For this reason,  some practitioners argue that microservices are a particular way of implementing of SOAs \cite{zimmermann2017microservices}, and therefore do not represent a new architectural style. We can say that microservices are a way of designing service-oriented architectures. In this study, we focus on microservices.

\section{Microservices}\label{s:microservice_principles}
Microservices are described by \citeauthor{newman2015building} \cite{newman2015building} as \textit{"small autonomous services that work together, modelled around a business domain"}. Microservices work independently from each other to ensure that a change in one service can be made without affecting other services. \citeauthor{dragoni2017yesterday} \cite{dragoni2017microservices} defines a microservice as \textit{a cohesive, independent process interacting via messages}. The term \textit{cohesive} indicates that the service only implements functionalities that are strongly related to each other. The microservices architecture is built around a few basic principles \cite{dragoni2017microservices}:

\begin{itemize}
    \item \textbf{Bounded context.} The bounded context, introduced by \citeauthor{evans2004domain} \cite{evans2004domain}, says that each microservice should implement a single business capability. This way, the system structure and business are perfectly aligned.
    \item \textbf{Size.} The microservices have to be small-sized. The small size brings major benefits in terms of maintainability and extendability. However, determining the right level of granularity appears to be a complicated issue \cite{di2018migrating, kalske2017challenges, newman2015building}. In \cite{newman2015building, selmadji2018re}, they refer to the small size as the single responsibility principle (SRP). The SRP states that each service should be focused only on one functionality.
    \item \textbf{Independency.} Microservices have to operate independently from each other. Independence between microservices is achieved by following the loose coupling and high cohesion principle. Furthermore, communication between microservices should only be through published interfaces. In \cite{selmadji2018re}, each microservice should also maintain its own database. 
\end{itemize}

Notice that more microservice principles exist such as being \textit{technology neutral} or \textit{automatically deployable} \cite{selmadji2018re}. However, these are not related to the structure and behaviour of microservices and therefore considered irrelevant for this study. \par

\section{Extracting data from the monolith}\label{s:information_views}
To make a proper decomposition of the monolith, we must have a sufficient amount of information about the monolith. We need to know its structure and how it behaves to define good microservice boundaries. There are various ways to collect relevant information. A literature study by \citeauthor{ponce2019migrating} on decomposition techniques outlines three categories: \textit{model-driven}, \textit{static analysis} and \textit{dynamic analysis}. Next to this, we also consider a fourth category called \textit{evolutionary analysis}. The static analysis is divided into \textit{dependency analysis} and \textit{semantic analysis}. This section is structured according to this division.

\subsection{Model analysis}
Model-driven approaches use system \textit{design elements} as input for the decomposition. System design elements provide documentation about the system and can be represented in many different ways, e.g., domain entities, use-case diagrams or data flow diagrams. The existence of design elements is not guaranteed as it depends on the company. Different companies might use different modelling techniques. For this reason, \citeauthor{gysel2016service} \cite{gysel2016service} developed a technique that allows nine different representations of the system as input. If multiple design elements are available, it is also possible to combine them. To illustrate this better, we give an example based on class diagrams. Class diagrams represent both the structural and behavioral  features of the system. A simple association relationship between two classes tells us there is some dependence. This dependence is described as \textit{semantic proximity} by \cite{gysel2016service} as the two have a semantic connection given the business domain. Two classes that have  strong semantic proximity should be modelled in the same microservice. In the end, the resulting information extracted from the chosen design elements is represented in an undirected weighted graph. To automate the approach, they require the design elements to be in a machine-readable format. \par 
Another design element that can be used is a data-flow diagram (DFD). Data-flow diagrams give a good representation of the business processes of the monolith \cite{chen2017monolith}. A DFD shows how the data flows through the system. Having this knowledge, we can discover, e.g. functions that make use of the same datastore and thus have some relation. Two approaches that use data-flow diagrams for deriving microservices are \cite{chen2017monolith, selmadji2020monolithic}. It is also possible to use business processes as input for the decomposition. In \cite{daoud2020towards} business processes modelled in BPMN notation are used to extract control (execution order), semantic (functional similarity), data (information sharing) and organisational (cross-functional operations) dependencies. \par 
A limitation of model-based approaches is that the used artefacts or documentation can be unavailable or not up to date.

\subsection{Dependency analysis}
In a dependency analysis, structural code dependencies are extracted from the source code. Structural code dependencies capture direct dependencies between two code entities \cite{beck2011congruence}. An example of such direct dependency is: method A calls method B, or class X extends class Y. Often, structural dependencies are represented in a directed graph (also called call graph) \cite{alsarhan2020software}. The dependency graph consists of nodes and edges. The nodes represent the system's modules (e.g. files, classes, functions) while the edges represent their relationship (e.g. function calls, inheritance relationship). The dependency graph is also able to show the strength of the relationship between two code entities. This strength is expressed in weight terms. In \cite{matias2020determining} the weights are calculated by looking at the number of imports and method calls between two components.\par
The structural dependencies might also be valuable to discover indirect relationships among code entities. This can be done by considering the structural dependencies of a code entity (e.g. function) as a feature vector. The rationale behind this is that the similarity between two feature vectors indicates that the two code entities have a similar purpose or functionality. These indirect dependencies are also called the \textit{fan-out similarity (FO)} \cite{beck2011congruence}.\par
A limitation of techniques that focus on structural dependencies is that they rely on a strong assumption that the system is well designed according to software engineering principles. However, for many systems, this is not the case \cite{saeidi2015search}.

\subsection{Semantic analysis}
In semantic analysis, the source code is interpreted as plain text documents \cite{beck2011congruence}. For each code file, a vocabulary is built representing the linguistic information that is embedded in the source code, such as identifier names and comments. This information could be relevant, assuming that similar vocabularies indicate some degree of relation. \par
Each code file is modelled as a bag-of-words vector that shows the occurrence of each term in a document. All the documents together represent a term-document matrix. A successful technique that analyses relationships between documents in the term-document matrix is Latent Semantic Indexing (LSI). In LSI, the term-document matrix is weighted to balance out scarce and ubiquitous terms. Next, singular value decomposition (SVD) is used to reduce the vector space model's dimensions. The similarity in a LSI vector space model is typically defined as the cosine between two vectors. The LSI allows us to compute document-document, term-document and term-term similarities. \par
A risk of semantic analysis is that its success heavily depends on the quality of source code naming \cite{saeidi2015search, alsarhan2020software}. The analysis does not work when developers do not comply with comments and naming conventions. Some companies even anonymise identifier names in the source code to deal with security issues.

\subsection{Dynamic analysis}
In dynamic analysis, the system functionalities are analysed at runtime. To analyse the system at runtime, operational data represented in log files are necessary. A log (execution) file contains information such as a timestamp, the invocated function and its arguments and an optional message. The information obtained from the analysis tells us how the dependencies are exercised during executions and how the system works in reality. For example, we can discover how frequent two function call each other and thereby determine the strength of their relationship. Furthermore, because of the timestamp in the execution log, we know in which order functions are executed. We could use this information when assuming that functions executed close to each other have some relationship. The extracted information from dynamic analysis could be represented in a weighted dependency graph. It is also possible to use the information to update the weights in the dependency graph derived from static analysis \cite{matias2020determining}. \par
A possible limitation of dynamic analysis is that changes in the source code are required when operational data is not automatically captured \cite{alsarhan2020software}. To let systems record execution logs, they need to be instrumented and redeployed again. However, when the new instrumented system could not run for a sufficiently long time, there is a risk that the log files do not represent all aspects of the system. To cope with this problem, it is important to have a suitable set of test cases available that simulate as many aspects of the system as possible. 

\subsection{Evolutionary analysis}
Evolutionary analysis aims to extract relevant information that is paired with the evolution of software \cite{beck2011congruence}. This means that we look at how the software changes over time. When two code entities appear to frequently change together during development, this reveals an implicit relationship. Evolutionary data is often managed by a version control system (VCS) such as Git. In version control systems, the change history of a particular project is captured revision logs. A revision log contains information about the code change, such as the author, date and changed files. \par 
Analysing revision logs gives us important insights into the relationships between code entities. For example, as mentioned before, we could identify code entities that frequently change together. Furthermore, since the author who changes the code is known, we can also look at the ownership of code \cite{beck2011congruence}. When two code entities are owned by the same author(s), they might be covered by the same expertise and thus represent some relationship. \par
A limitation of the evolutionary analysis is that change information is not always properly documented. Furthermore, to cover all the parts of the system, a huge set of evolutionary data is necessary \cite{alsarhan2020software}. Dealing with this amount of data is a challenging task. 

\section{Clustering algorithms}\label{s:clustering_algorithms}
Based on the extracted data mentioned in the previous sections, the software has to be clustered into a set of microservices. Clustering is an unsupervised learning task in which data points that are closely related to each other are grouped together. In this section, we describe three classes of clustering algorithms that are commonly used in the process of decomposing monolithic software, namely: \textit{graph-based clustering}, \textit{hierarchical clustering} and \textit{genetic-based clustering}.

\subsection{Graph-based clustering}
Graph-based algorithms partition an undirected graph into sub-graphs. This way, the algorithm does not start from individual data points but tries to find sub-graphs according to some algorithm-specific criteria \cite{wiggerts1997using}. This means that the sub-graphs are forming the clusters. In these graphs, each node represents a code entity, and an edge represents their relationship. Graph-based clustering can be seen as a kind of \textit{partitioning method} since it relocates instances from one cluster to another, starting from an initial partitioning \cite{rokach2005clustering}. \par
To achieve global optimisation in graph-based clustering, an exhaustive search of all possible partitions is required. However, since this is often not feasible due to the large search space, greedy search heuristics are often used \cite{rokach2005clustering}. \par 
A widely used graph-based clustering technique is the Girvan-Newman algorithm \cite{girvan2002community}. This algorithm is often used to identify microservices \cite{gysel2016service, matias2020determining}. In \cite{lohnertz2020steinmetz}, seven graph-based clustering algorithms are compared on the task of decomposition the monolith. Another graph-based clustering technique that is used in previous decomposition studies is the minimal spanning tree (MST) \cite{mazlami2017extraction}. 
An advantage of graph-based algorithms is that they do not require to set the number of clusters.

\subsection{Hierarchical clustering}
Hierarchical clustering is a non-stochastic technique that builds clusters by recursively partitioning instances in either top-down (divisive) or bottom-up (agglomerative) fashion \cite{rokach2005clustering}. In \textit{agglomerative hierarchical clustering}, each data point (code entity) start with its own cluster (singletons). Then, in each step, the two most similar clusters are successively merged until the desired cluster structure is obtained. On the other hand, in \textit{divisive hierarchical clustering} all the data points are represented in one cluster. In each step, the cluster is divided into suitable subclusters, until again a desired cluster structure is obtained. The result of a hierarchical algorithm is typically visualised in a \textit{dendrogram}. To obtain clusters from the dendrogram, a \textit{cutting point} needs to be determined. Note that different cutting points will lead to different clusters. Agglomerative hierarchical algorithms are more often used than divisive algorithms. This is because it is infeasible to consider each possible divisions from the initial cluster. The complexity of hierarchical algorithms depends linearly on the number of classes in the system \cite{kebir2012comparing}.\par 
An advantage of hierarchical clustering is that the algorithm reveals a hierarchy within the data. This information is often more insightful compared to 2-D clusters. Furthermore, the algorithm does not need any apriori information regarding the number of clusters. 

\subsection{Genetic-based clustering}
Genetic algorithms (GA) belong to the class of evolutionary algorithms (EA) and are inspired by concepts of natural selection such as mutation, crossover and selection. Even though there are many variations of GAs, the overall process stays the same. Genetic algorithms start with randomly initialising a \textit{population} of candidate solutions. In GA terminology, a candidate solution is called an \textit{individual} and the set of individuals in a particular iteration is considered a \textit{generation}. After obtaining the initial generation, an iterative process starts in which the fitness of every individual is evaluated based on a predefined \textit{objective function}. A new generation is created by randomly taking the fittest individuals of previous generations. This selection is based on evolution theory, in which only the strongest species survive. The selected individuals for the next generation are modified by applying GA operators such as \textit{crossover} and \textit{mutation}. Typically, the GA stops when a maximum number of generations is produced or when a satisfactory fitness level has been reached for the population. \par
Genetic algorithms are considered beneficial when there is a multi-objective optimisation problem. They perform surprisingly good in highly constrained problems, where the number of good solutions is relatively small compared to the search space \cite{Doval1999}.
