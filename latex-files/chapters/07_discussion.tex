This chapter discusses the results and implications made from the results. We first discuss the results. After this, we discuss validity threats, limitations, and future directions for research.\par

\section{Interpretations}
This section is divided into two parts. We first discuss findings related to the use of individual streams of input data. After this, we discuss the findings regarding the use of multiple views of the system.

\subsection{Individual view as input}
Regarding the independence of functionality for decompositions made of a single source of information, the CHD and CHM scores show the most promising results when only semantic data is involved. The second best CHD and CHM results are achieved for decompositions constructed with static data. The CHD and CHM measure the functional cohesion within services based on the rationale that services should provide coherent functionality to their external clients. This means the decompositions made with only semantic information provide the most coherent functionality to other services when comparing it to static and dynamic decompositions.\par

When looking at the functional independence metrics that compute the coupling of the decomposition, the static decomposition shows the most promising results. The IFN and the OPN scores are both the lowest for decompositions made with static data when only using an individual source of information. The results are close to the ones obtained from dynamic decompositions but very different from the semantic decompositions. The semantic decomposition shows a significant increase in the IFN and OPN rate compared to the static and dynamic decompositions. However, we think this could be justified by the fact that semantic decompositions do not take into consideration calling relationships between code fragments. As a result, code fragments that invocate each other but have a very different lexicon are likely to be clustered in different services. \par

The high OPN score for the semantic decompositions also results in a worse SMQ score. This makes sense because more operations mean more external connections between services are made.\par

Regarding IFN, we observed that decompositions made with semantic information have tended to choose less but bigger services resulting in a worse IFN. A worse IFN means that more interfaces are included in less services, which results in bigger services. In related papers \cite{brito2021identification}, a higher IFN rate results in bigger services and thus less communication between services. Less communication between services subsequently results in a better SMQ rate. However, in our decomposition made with semantic information, we do not see this pattern. Even though they have a higher IFN rate, the SMQ rate is not better. We think that the reason for this is because code fragments that do not call each other can be clustered in the same service because of their similar lexicon. \par

To cope with this, existing approaches \cite{brito2021identification, lohnertz2020steinmetz} always take the structural dependencies as the underlying structure of the graph. The edge weights of the graph are then updated to account for additional information such as semantic or dynamic dependencies. \par

The high OPN and low SMQ score are also reflected by \citeauthor{jin2019service} \cite{jin2019service}. In their paper, they compare their proposed technique that is based on dynamic information with a technique that only incorporates semantic information. In their results, they indicate that the IFN rates of the decomposition made with semantic information only are much higher. Even though these tools are different, it still shows the same pattern as we observe in our data. \par

\subsection{Multi-views as input}
Regarding the decompositions that incorporate multiple views of the system, there is not an obvious pattern in the data. Our expected outcome, that multiple views of the system would result in a better quality of the decomposition is not reflected by the data. Only the CHD score achieved the highest value on average when all information streams of the system are included.\par

The decompositions resulting from static and semantic information as input are for all quality metrics (on average) at least as good as the worst performing individual group. This is also true when combining semantic and dynamic data, except for the SMQ metric. Decompositions constructed with static and dynamic information sources perform at least as good as the worst individual source except for the CHM and OPN metric. \par

Regarding the CMQ and SMQ results for decomposition constructed with multiple views of information, we observe a much lower SMQ score and much higher CMQ score when semantic edges are incorporated. In other words, we see the SMQ rate increasing and the CMQ decreasing when semantic information is left out. However, we note that the decrease in CMQ is less significant than the increase in SMQ. Having said this, we state that static and dynamic information sources are able to create decompositions with reasonable CMQ scores, while semantic data is not able to create decompositions with high SMQ scores. \par

Higher values of CMQ compared to SMQ when semantic information is included could be justified by the high amount of semantic edges in the graph. The edges that are present in a combined graph with semantic information are more likely to represent a semantic edge than a static or dynamic edge. This is because the semantic analyser touches more aspects of the system and thus creates more dependencies. This is also reflected by the average coverage rate given in Table \ref{tab:results_coverage}. For this reason, we think the algorithm will privilege code fragments that are semantically closer, as more semantic dependencies exist compared to static or dynamic edges. This also explains the less strong changes in the results for PyPetstore, which is a relatively small application compared to the others. Since the application is much smaller, more edges are represented by both information streams, and therefore the algorithm favors both information sources more equally. \par

As mentioned before, to cope with this, one could make the underlying structure of the graph only represent structural dependencies. Additional information is then added by changing the edge weights of the static edges in the graph.

\section{Limitations and validity threats}
One threat to the validity of the research is the low amount of sample projects used to validate the work. Due to time constraints, we were able to validate the approach on only seven different open-source systems. Another reason for this low amount of input projects is the difficulty of obtaining dynamic data. While semantic and static data can be relatively easily obtained with our tool, dynamic requires much more effort. This is because we need to actually run the input application on different test scenarios. This requires the application to have sufficient test scenarios available, which is not always the case. To enhance the reliability and generalizability of the results, we need to validate the approach on more systems. \par
Moreover, the approach is only tested on open-source Python applications. This means it is hard to guarantee that the evaluation results can be generalised to applications with different technology stacks. Another threat relates to the fact that we only use open-source projects. It may be possible that results vary for proprietary software.\par

Another threat to the internal validity of the research is related to the effect of the clustering algorithm on the change in the results. Even though this assumption is verified on a small experiment, we cannot guarantee that the same results are obtained for every possible clustering algorithm. To generalise the results, more testing is necessary to understand the real impact of the clustering algorithm on the resulting microservice decompositions. \par

Our multi-view clustering tool that generates static, semantic, and dynamic edges from a Python project will be publicly available. Also, the data that is obtained for the analysed projects are uploaded to our repository\footnote{\href{https://github.com/larsvasseldonk/thesis}{https://github.com/larsvasseldonk/thesis}}. This way, the research is easily reproducible and can be extended if wanted. \par

Another validity threat is the impact of the coverage on the quality of the decomposition. The coverage of an information stream might influence changes in the result. However, during this research, we did not experiment with different degrees of coverage to understand how it affects the quality of the decomposition. \par

The log files that are used to construct dynamic dependencies are artificially created. This means the log files, and thus dynamic dependencies do not reflect real world behaviour of the system. The reason for this is because we only worked with open-source projects and therefore did not have access to real-world log files. The artificial log files are generated by running the test scenarios available for the project. \par

Another tread relates to the quality of the resulting microservice decompositions. This is because it is theoretically possible, however unlikely, to achieve good metrics that do not necessarily represent good decompositions of the system. The number of projects used for validation should decrease such possibilities. However, a qualitative analysis of the candidate microservices by experts is necessary to make any further conclusions. Bringing experts into the validation process would help to understand the quality of the decompositions and help identifying possible improvements in the approach.

\section{Future work}
In this section, we discuss future research directions. \par

At first, future work can extend the validation process by applying the tool to more monolithic systems. It would be valuable to know how the tool performs on real-world applications that include real log files. Furthermore, the resulting decompositions should be validated with a qualitative analysis. In future work, one can incorporate experts into the validation process to even better understand the differences between the decompositions. Another direction for future research is to validate the approach with systems that have a monolith and microservices version available. The monolithic version is then used as input for the approach, and the resulting decomposition is compared to the decomposition present in the microservice version. However, to do this, it is necessary for the system to have both a monolith and microservices version, which we, unfortunately, did not discover. \par

Future work can also study the impact of different levels of abstraction in the decomposition. In this thesis, we cluster Python programs at module and class level, where a module contains functions that are not defined inside a class. However, to get a finer decomposition, future research can study the impact of decompositions made at method and function level. \par

Lastly, additional views of the system can be added to the tool to find out how it further affects the final decomposition. An example of this is the revision history. The revision history can be used to compute evolutionary coupling between code fragments. Furthermore, the tool can be extended with a visualising module to more easily inspect the resulting decomposition. This also makes it easier to evaluate the candidate microservice qualitatively. By making the visualisation  interactive, one can also research how experts manually change the achieved decomposition based on their domain knowledge.